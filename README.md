# AI and ML researcher

## Education 
 - Doctor of Philosophy in Information Science (Dual degree), Japan Advanced Institute of Science and Technology (JAIST), Japan (SIIT-JAIST-NSTDA Scholarship)
 - Doctor of Philosophy in Engineering and Technology (Dual degree), The Sirindhorn International Institute of Technology (SIIT), Thailand (SIIT-JAIST-NSTDA Scholarship)
- Master of Science in Information and Technology, Panyapiwat Institute of Management, Thailand (NSTDA Scholarship)
- Bachelor of Science in Information and Technology, Thai-Nichi Institute of Technology, Thailand

## Publications

![Parametric_SR](https://supattavir.github.io/asset/image/Parametric_SR_thumb.png)
**Parametric Loss based Super-Resolution for Scene Text Recognition**<br>
S. Viriyavisuthisakul, N. Kaothanthong, P. Sanguansat, T. Racharak, Minh Le Nguyen, C.Haruechaiyasak , and T. Yamasaki.<br>
ECCV, 2024<br>
[arxiv](link_to_foundpose_arxiv) / [project page](link_to_foundpose_project_page) / [code](link_to_foundpose_code)

| <img src="https://supattavir.github.io/asset/image/Parametric_SR_thumb.png" width="200"> | **Parametric Loss based Super-Resolution for Scene Text Recognition**<br> 
S. Viriyavisuthisakul, N. Kaothanthong, P. Sanguansat, T. Racharak, Minh Le Nguyen, C.Haruechaiyasak , and T. Yamasaki.<br> ECCV, 2024<br> [arxiv](link_to_foundpose_arxiv) / [project page](link_to_foundpose_project_page) / [code](link_to_foundpose_code) |
|:---:|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

<table>
  <tr>
    <td valign="top" width="30%">
      <img src="https://supattavir.github.io/asset/image/Parametric_SR_thumb.png" alt="FoundPose Paper" width="200">
    </td>
    <td valign="top" width="70%">
      **Parametric Loss based Super-Resolution for Scene Text Recognition**<br>
      S. Viriyavisuthisakul, N. Kaothanthong, P. Sanguansat, T. Racharak, Minh Le Nguyen, C.Haruechaiyasak , and T. Yamasaki.<br>
      [arxiv](link_to_foundpose_arxiv) / [project page](link_to_foundpose_project_page) / [code](link_to_foundpose_code)
    </td>
  </tr>
  <tr>
    <td valign="top" width="30%">
      <img src="link_to_echoscene_image.png" alt="EchoScene Paper" width="200">
    </td>
    <td valign="top" width="70%">
      **EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion**<br>
      Guangyao Zhai, Evin Pınar Örnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, Benjamin Busam<br>
      ECCV, 2024<br>
      [arxiv](link_to_echoscene_arxiv) / [project page](link_to_echoscene_project_page) / [code](link_to_echoscene_code) / [data](link_to_echoscene_data)
    </td>
  </tr>
</table>

<br>

[![EchoScene Paper](link_to_echoscene_image.png)](link_to_echoscene_project_page)
**EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion**<br>
Guangyao Zhai, Evin Pınar Örnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, Benjamin Busam<br>
ECCV, 2024<br>
[arxiv](link_to_echoscene_arxiv) / [project page](link_to_echoscene_project_page) / [code](link_to_echoscene_code) / [data](link_to_echoscene_data)


